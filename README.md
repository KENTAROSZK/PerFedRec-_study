
# PerFedRec++: 自己教師あり事前学習によるパーソナライズされた連合レコメンデーションの強化

[![Paper PDF](https://img.shields.io/badge/Paper-PDF-blue?style=flat&link=https%3A%2F%2Farxiv.org%2Fpdf%2F2305.06622.pdf)](https://arxiv.org/pdf/2305.06622.pdf)

このプロジェクトは、**PerFedRec++** モデルのPyTorch実装です。PerFedRec++は、パーソナライズされた連合レコメンデーションシステムを強化するために提案された新しいフレームワークです。

## 概要 (Introduction)

今日のデジタル世界では、AmazonやYouTubeのようなプラットフォームで「次に何を見るか」「何を買うか」を提案してくれる**レコメンデーションシステム**が非常に重要です。しかし、これらのシステムがユーザーの好みや行動に関する大量のデータを収集・利用するにつれて、**ユーザープライバシー**に関する深刻な懸念が高まっています。GDPRやCCPAのような規制も、このプライバシー保護の必要性から生まれました。

このプライバシーの課題に対処するため、**連合学習 (Federated Learning, FL)** という有望なパラダイムが登場しました。連合学習は、生のユーザーデータを中央サーバーに送信するのではなく、**モデルの更新情報のみを交換**することでユーザーデータのプライバシーを保護します。レコメンデーションシステムと連合学習を組み合わせたものが**連合レコメンデーションシステム**です。

現在の連合レコメンデーションシステムは、いくつかの大きな課題に直面しています:
1.  **異質性とパーソナライゼーション**: ユーザーの属性やローカルデータの多様性のため、個々のユーザーに合わせたモデル（パーソナライズされたモデル）が必要です。
2.  **モデル性能の劣化**: 擬似アイテムラベリングや差分プライバシーなど、プライバシー保護のためのメカニズムがモデルの性能を低下させる可能性があります。
3.  **通信のボトルネック**: 標準的な連合レコメンデーションアルゴリズムは、サーバーとデバイス間の通信オーバーヘッドが高くなる傾向があります。

**PerFedRec++** は、これらの課題を同時に解決するために提案された新しいフレームワークです。

## PerFedRec++の仕組み (How PerFedRec++ Works)

PerFedRec++は、主に以下の3つの主要なモジュールで構成されています。

### 1. 自己教師あり事前学習モジュール (Self-Supervised Pre-Training Module)

*   **データ拡張の利用**: 連合レコメンデーションシステムのプライバシー保護メカニズム（例: クライアント選択、擬似アイテムラベリング、差分プライバシー）自体が、システムに関する異なる「ビュー」を生成すると捉えられます。これは、グラフ学習におけるノードドロップアウトやエッジ摂動、ノイズ注入といったデータ拡張の手法に似ています。
*   **対照学習 (Contrastive Learning)**: 生成されたこれら2つの拡張されたグラフビューを**対照タスク**として使用し、**自己教師ありグラフ学習**によってモデルを事前学習させます。具体的には、対照損失 (contrastive loss) を用いて、同じノードの異なるビュー間の類似度を最大化します。
*   **効果**: この事前学習により、埋め込みの表現がより統一され、モデルの性能が向上します。また、連合学習の初期状態を改善することで、モデルの収束が速くなり、全体の訓練にかかる時間と**通信負荷を軽減**します。

### 2. ユーザーサイドのローカルレコメンデーションネットワーク (User-Side Local Recommendation Network)

*   **埋め込み層**: 各ユーザーとアイテムは、それぞれ密な埋め込みベクトルに変換されます。事前学習された埋め込みが初期値として使われます。
*   **ローカルGNNモジュール (Local Graph Neural Network)**: 各ユーザーは自身のプライベートなユーザー・アイテムインタラクション情報（履歴データ）を用いて**ローカルGNN**を訓練します。アイテム埋め込みはサーバーを通じて共有されますが、ユーザー埋め込みはプライバシー保護のためローカルに保持されます。
*   **パーソナライズされた予測**: 最終的に、各ユーザーは**グローバルな連合モデル**、**クラスタレベルの連合モデル**、そして**自身のファインチューンされたローカルモデル**を組み合わせて、パーソナライズされたレコメンデーションモデルを獲得します。
*   **プライバシー保護**: 勾配を保護するため、**ローカル差分プライバシー (Local Differential Privacy, LDP)** が適用されます。これは、勾配にランダムなノイズを加えることで実現されます。

### 3. サーバーサイドのクラスタリングに基づく連合 (Server-Side Clustering-Based Federation)

*   **ユーザーのクラスタリング**: サーバーは、各ユーザーの学習された埋め込み (表現) に基づいて、ユーザーを類似した**K個のグループにクラスタリング**します。K-meansのような一般的なクラスタリング手法が利用されます。
*   **モデルの集約**: サーバーは、参加するすべてのユーザーからの重み付けされた合計によって**グローバルモデル**を統合します。さらに、各クラスタ内のユーザーからの重み付けされた合計によって、**クラスタレベルのモデル**も集約します。これらのモデルは、パーソナライズされたレコメンデーションのために各ユーザーに提供されます。
*   **ユーザー選択**: 通信コストが高いシナリオでは、各クラスタ内でクラスタサイズに比例してランダムにユーザーを選択する**クラスタベースのユーザー選択メカニズム**も導入されています。

## 実験結果 (Experimental Results)

PerFedRec++は、3つの実世界データセット（**Yelp、Amazon-Kindle、Gowalla**）で広範な実験を行い、その有効性と効率性を検証しました。

*   **優れた性能**: PerFedRec++は、既存の最先端の連合レコメンデーションシステムと比較して、すべてのデータセットで**優れた性能**を達成しました。特に、先行研究であるPerFedRecと比較して、RecallとNDCGで平均11.92%と11.68%の相対的な改善が見られました。
*   **自己教師あり事前学習とパーソナライゼーションの寄与**: アブレーションスタディにより、自己教師あり事前学習とパーソナライゼーションモジュールの両方が、全体の性能向上に大きく貢献していることが示されました。
*   **訓練効率の向上**: 自己教師あり事前学習は、より良い初期状態を提供することで、連合訓練の**収束を速め**、通信オーバーヘッドを削減することが確認されました。

## リポジトリの使い方 (Repository Usage)

このリポジトリは、PerFedRec++モデルを動かすためのコードと設定ファイルを提供しています。

### プロジェクト構造 (Project Structure)

```
.
├── LICENSE
├── PerFedRec++
│   ├── SELFRec.py
│   ├── base
│   │   ├── graph_recommender.py
│   │   └── recommender.py
│   ├── conf
│   │   ├── FedGNN.conf
│   │   ├── FedMF.conf
│   │   ├── LightGCN.conf
│   │   ├── MF.conf
│   │   ├── PerFedRec.conf
│   │   ├── PerFedRec_plus.conf  <- PerFedRec++の設定ファイル
│   │   ├── SGL.conf
│   │   ├── SimGCL.conf
│   │   └── XSimGCL.conf
│   ├── data
│   │   ├── augmentor.py
│   │   ├── data.py
│   │   ├── graph.py
│   │   ├── loader.py
│   │   └── ui_graph.py
│   ├── dataset
│   │   ├── yelp_test             <- Yelpデータセットの例
│   │   │   ├── test.txt
│   │   │   ├── train.txt
│   │   │   └── valid.txt
│   ├── main.py                   <- 実行スクリプト
│   ├── model
│   │   └── graph
│   │       ├── FedGNN.py
│   │       ├── FedMF.py
│   │       ├── LightGCN.py
│   │       ├── MF.py
│   │       ├── PerFedRec.py
│   │       ├── PerFedRec_plus.py <- PerFedRec++のモデル実装
│   │       ├── SGL.py
│   │       ├── SimGCL.py
│   │       └── XSimGCL.py
│   ├── result_vis
│   │   ├── result_vis.py
│   │   ├── result_vis_ali.py
│   │   ├── result_vis_clu.py
│   │   └── result_vis_uni.py
│   └── util
│       ├── algorithm.py
│       ├── conf.py
│       ├── evaluation.py
│       ├── logger.py
│       ├── loss_torch.py
│       └── sampler.py
├── README.md
└── fig
    ├── fig1.png
    └── fig2.png
```


### 実行方法 (How to Run)

`main.py`スクリプトを使用して、モデルの訓練と評価を実行できます。

**コマンドライン引数**:

*   `--model`: 使用するモデル名を指定します。`PerFedRec_plus`を指定してください。
    *   利用可能なグラフベースラインモデル：`LightGCN`, `MF`, `FedGNN`, `FedMF`, `PerFedRec`, `PerFedRec_plus`。
    *   利用可能な自己教師ありグラフモデル：`SGL`, `SimGCL`, `XSimGCL`。
*   `--dataset`: 使用するデータセットを指定します。`kindle`、`yelp`、`gowalla`のいずれかです。
    *   内部では、`kindle`は`kindle_test`に、`yelp`は`yelp_test`に変換されます。
*   `--emb`: 埋め込みのサイズを指定します。デフォルトは`64`です。
*   `--pretrain_epoch`: 事前学習のエポック数を指定します。デフォルトは`5`です。
*   `--noise_scale`: ノイズのスケール（差分プライバシー関連）を指定します。デフォルトは`0.1`です。
*   `--clip_value`: 勾配クリッピングの値（差分プライバシー関連）を指定します。デフォルトは`0.5`です。
*   `--pretrain_noise`: 事前学習時のノイズのスケールを指定します。デフォルトは`0.1`です。
*   `--pretrain_nclient`: 事前学習時に参加するクライアントの数。デフォルトは`256`です。

**実行例**:

```bash
# YelpデータセットでPerFedRec++モデルを実行
python main.py --model PerFedRec_plus --dataset yelp

# Amazon-KindleデータセットでPerFedRec++モデルを実行し、埋め込みサイズを128に設定
python main.py --model PerFedRec_plus --dataset kindle --emb 128

# GowallaデータセットでPerFedRec++モデルを実行し、事前学習エポック数を10に設定
python main.py --model PerFedRec_plus --dataset gowalla --pretrain_epoch 10
```

これにより、指定されたモデルが訓練され、結果が`./resultsX/`ディレクトリ（設定ファイル内の`output.setup`で指定）に出力されます。

## 引用 (Citation)

もしPerFedRec++があなたの研究やアプリケーションで役立った場合、以下の論文を引用してください:

```bibtex
@article{luo2023perfedrec++,
  title={PerFedRec++: Enhancing Personalized Federated Recommendation with Self-Supervised Pre-Training},
  author={Luo, Sichun and Xiao, Yuanzhang and Zhang, Xinyi and Liu, Yang and Ding, Wenbo and Song, Linqi},
  journal={arXiv preprint arXiv:2305.06622},
  year={2023}
}
```


---

はい、PerFedRec++フレームワークの各主要コンポーネントについて、知識がない方にも理解できるように、より詳しく、なぜその方法が採用されているのか、どのように機能するのか、そしてサンプル計算例を交えながらご説明します。

---

### PerFedRec++ フレームワークの主要コンポーネント

PerFedRec++は、ユーザーのプライバシーを保護しながら、より効果的なレコメンデーションを提供する新しい手法です。従来のレコメンデーションシステムが直面する、ユーザーデータの**プライバシー懸念**、ユーザー間のデータの**多様性（ヘテロジニティ）**、そしてモデル訓練の際の**通信コストの高さ**という3つの大きな課題に対処することを目指しています。このシステムは、主に3つのモジュールで構成されています。

#### 1. 自己教師あり事前学習モジュール (Self-Supervised Pre-Training Module)

このモジュールは、モデルの性能を向上させ、訓練の収束を速くするために、メインの連合学習の前にモデルを準備する「事前学習」の段階です。

*   **データ拡張の利用：**
    連合レコメンデーションシステムでは、ユーザーのプライバシーを守るために様々な仕組みが導入されています。PerFedRec++は、これらのプライバシー保護の仕組み（例えば、訓練に参加するユーザーの選択、架空のアイテムの追加、データにノイズを加えることなど）を、モデルを「訓練」するための**データ拡張**として利用するという画期的なアイデアを提案しています。

    これは、まるで同じ風景を異なる角度や照明で撮影し、それらを比較して風景そのものについて深く学ぶようなものです。これにより、モデルはより**堅牢（ロバスト）**なデータ表現を学習できます。

    *   **ノードドロップアウト（クライアント選択）：**
        *   **なぜ？** 連合学習では、通信コストを削減するため、通常、訓練ラウンドごとに全ユーザーの中から一部のユーザーだけが選択されて参加します。
        *   **どのように機能するのか？** この「一部のユーザーが参加しない」という行為が、グラフにおける「ノードドロップアウト」に似ていると捉えられます。グラフからランダムに一部の「ノード」（ここではユーザー）が一時的に削除された「ビュー」が生成されると考えるのです。モデルは、データの一部が欠落している状況でも、ユーザーやアイテムの本質的な特徴を捉えるように学習します。
        *   **サンプル計算例（イメージ）：**
            もし全ユーザーの集合 `V = {ユーザー1, ユーザー2, ユーザー3, ユーザー4}` があったとして、ある訓練ラウンドでランダムに**ユーザー1とユーザー3**が選択されず（ドロップアウトされ）、残りのユーザー2とユーザー4だけが参加したとします。このとき、システムは「ユーザー1とユーザー3の情報が欠落したグラフビュー」を自動的に生成したと見なされます。別のラウンドでは、別のユーザーがドロップアウトされるかもしれません。モデルは、このように一部の情報が欠落した複数のビューから学習することで、どのユーザーが参加しても、その情報の本質を捉える能力を高めます。

    *   **エッジ摂動（擬似アイテムラベリング）：**
        *   **なぜ？** ユーザーが実際に操作したアイテム情報（ユーザー・アイテムインタラクション）は非常にプライベートな情報であり、そのままサーバーに送ることはできません。
        *   **どのように機能するのか？** そこで、「擬似アイテムラベリング」という技術が使われます。これは、ユーザーが実際にインタラクションしていない**架空のアイテム**をランダムに選び、あたかもインタラクションしたかのように見せかけて（マスクして）サーバーに送信する技術です。これはグラフにおける「エッジ摂動」、つまりグラフの接続をランダムに変更するのと似ています。これにより、攻撃者が本物のユーザー行動を推測することがより困難になります。
        *   **サンプル計算例（イメージ）：**
            ユーザーAがアイテムXとYにインタラクションしたとします。本来ならこの情報のみが処理されますが、プライバシー保護のためにランダムに選ばれた「擬似アイテムZ」もユーザーAがインタラクションしたかのようにマークして送ります。つまり、実際のデータは「ユーザーA - アイテムX, アイテムY」ですが、システムではこれを「ユーザーA - アイテムX, アイテムY, **擬似アイテムZ**」という形で認識します。これにより、モデルは実際のインタラクションと偽のインタラクションの区別を学習しつつ、ノイズに強い表現を構築します。

    *   **ノイズ注入（ローカル差分プライバシー）：**
        *   **なぜ？** モデルの更新情報（勾配）がサーバーに送信される際、わずかなノイズが加えられることがあります。これは「ローカル差分プライバシー（LDP）」というプライバシー保護技術の一部です。ノイズが加わっても、統計的な特性は保たれるため、正確な個人情報を特定されにくくします。
        *   **どのように機能するのか？** このノイズの追加は、自己教師あり学習における「ノイズ注入」というデータ拡張の手法と同じ効果をもたらします。ユーザーやアイテムの「埋め込み」（数値のベクトル表現）にランダムなノイズを加えることで、モデルはノイズがあっても安定した表現を学習できます。
        *   **サンプル計算例（イメージ）：**
            あるアイテムの埋め込みベクトルが `E_item = [0.1, 0.2, 0.3]` だったとします。これに、例えば乱数生成器が `[0.005, -0.01, 0.002]` というノイズを生成したとすると、実際に使われる埋め込みは `E'_item = [0.1+0.005, 0.2-0.01, 0.3+0.002] = [0.105, 0.19, 0.302]` となります。このわずかな変更により、元データからの逆算が難しくなりつつ、モデルは様々なバリエーションの入力に対応できるようになります。

*   **対照学習 (Contrastive Learning)：**
    *   **なぜ？** 上記のデータ拡張によって生成された異なる「ビュー」を利用して、モデルがより良い「埋め込み」を学習できるようにします。ここでいう「埋め込み」とは、ユーザーやアイテムの好みを数値のベクトルで表現したもので、このベクトルが似ていればいるほど、ユーザーやアイテムも似ていると判断できます。
    *   **どのように機能するのか？** PerFedRec++では、**自己教師ありグラフ学習**の一種である「対照学習」を導入します。これは、**同じユーザーやアイテムから生成された異なるビューは互いに似ているべき**であり、**異なるユーザーやアイテムのビューは互いに似ていてはならない**という考え方に基づいています。この「似ているもの同士を引き寄せ、似ていないもの同士を遠ざける」という学習を**対照損失 (contrastive loss)**を用いて行います。これにより、埋め込みの分布がより均一になり、モデルの性能が向上します。
    *   **サンプル計算例（イメージ）：**
        ユーザーAの元のデータから、ノードドロップアウトによって生成されたビュー1（**A'**）と、ノイズ注入によって生成されたビュー2（**A''**）があったとします。また、別のユーザーBのビュー1（**B'**）も存在します。
        対照学習の目標は、**A'とA''の間の類似度を最大化**することです（これらは「ポジティブペア」と呼ばれます）。同時に、**A'とB'の間の類似度を最小化**します（これらは「ネガティブペア」と呼ばれます）。
        「InfoNCE」という対照損失関数 を使うと、これは以下のような形で表現されます（簡略化された例）：
        `損失 = -log [ 類似度(A', A'') / Σ(類似度(A', 他のすべてのビュー)) ]`
        ここで、Σ(類似度(A', 他のすべてのビュー)) には、ポジティブペアである `類似度(A', A'')` と、ネガティブペアである `類似度(A', B'')` など、A'と他のすべてのビューとの類似度が合計されます。この損失を最小化することで、`類似度(A', A'')` が他のどの類似度よりも格段に高くなるようにモデルが学習します。
        結果として、ユーザーやアイテムの表現（埋め込み）がより**識別可能**になり、かつ**均一に分布**するようになります。

*   **効果：**
    この自己教師あり事前学習は、主に以下の2つの効果をもたらします:
    *   **モデル性能の向上：** 埋め込みの表現がより統一され、情報が密に詰まるようになるため、レコメンデーションの精度が上がります。
    *   **通信負荷の軽減：** 事前学習によってモデルが良い初期状態を持つため、連合学習の訓練がより速く収束します。訓練のエポック数（訓練の繰り返し回数）が減ることで、サーバーとクライアント間のデータ交換回数が減り、結果的に全体の通信量が大幅に削減されます。

#### 2. ユーザーサイドのローカルレコメンデーションネットワーク (User-Side Local Recommendation Network)

これは、各ユーザーのデバイス上で行われるモデル訓練のプロセスです。

*   **埋め込み層：**
    *   **なぜ？** ユーザーやアイテムのID（例えば、「ユーザー番号123」や「アイテム番号456」）は、そのままではコンピュータが理解できません。そこで、これらのIDを、数値の並び（ベクトル）である「埋め込み」に変換します。この埋め込みは、ユーザーの好みやアイテムの特徴を表現する「情報が凝縮された形」と考えることができます。
    *   **どのように機能するのか？** 各ユーザーとアイテムは、それぞれ独自の**密な埋め込みベクトル**に変換されます。この段階では、自己教師あり事前学習モジュールで得られた**埋め込みが初期値として利用されます**。これにより、訓練がゼロから始まるよりも効率的になります。重要なのは、アイテムの埋め込みはサーバーを介して他のユーザーと共有されますが、**ユーザー自身の埋め込みはプライバシー保護のため、そのユーザーのデバイス内に留められます**。

*   **ローカルGNNモジュール (Local Graph Neural Network)：**
    *   **なぜ？** レコメンデーションの精度を高めるためには、ユーザーとアイテムの関係だけでなく、ユーザー間の類似性やアイテム間の関連性などの**「構造情報」**を考慮することが重要です。グラフニューラルネットワーク（GNN）は、このような構造情報を捉えるのに非常に強力なツールです。
    *   **どのように機能するのか？** 各ユーザーは、自身のデバイスに保存されているプライベートなユーザー・アイテムインタラクション情報（過去の行動履歴）を用いて、**ローカルなGNNモデルを訓練します**。
        *   **プライバシー保護されたインタラクション共有：** ユーザーは、自分がインタラクションしたアイテムの「暗号化されたID」と「プライバシー保護された埋め込み」をサーバーにアップロードします。サーバーはこれらの暗号化された情報を他のユーザーに送り返すことで、各ユーザーは**他のユーザーの身元を明かすことなく**、彼らがどのアイテムとインタラクションしたかを（暗号化された形で）知ることができます。これにより、各ユーザーは自身のローカルなインタラクショングラフを、他のユーザーの情報を使って「拡張」することができます。
        *   **擬似アイテムラベリングとインタラクションアイテムマスキング：** これらは、さらなるプライバシー保護のための追加の技術です。
            *   **擬似アイテムラベリング:** 前述のように、ユーザーが実際にインタラクションしていないアイテムを、あたかもインタラクションしたかのように見せかけてデータを混乱させます。
            *   **インタラクションアイテムマスキング:** 訓練中に、実際にインタラクションしたアイテムの一部をランダムに「隠し」、インタラクションしていないものとして扱います。これにより、攻撃者が元のユーザー行動を推測することがさらに難しくなります。
        *   GNNは、これらの情報を使ってユーザー埋め込みとアイテム埋め込みを生成します。例えば、**LightGCN**のような軽量で効果的なGNN構造が利用されます。

*   **パーソナライズされた予測：**
    *   **なぜ？** ユーザーは一人ひとり異なる好みを持っています。全てのユーザーに同じレコメンデーションモデルを使うのではなく、それぞれのユーザーに合った「パーソナライズされた」モデルを提供することが、レコメンデーションの質を向上させる鍵です。
    *   **どのように機能するのか？** PerFedRec++では、各ユーザーは最終的に、3つの異なるレベルのモデルを組み合わせて、**自分専用のレコメンデーションモデル**を構築します。
        1.  **グローバル連合モデル：** システム全体の全参加ユーザーの情報を集約して作られた、最も一般的なモデル。
        2.  **クラスタレベル連合モデル：** サーバーによって類似のユーザーがグループ化（クラスタリング）され、そのクラスタ内のユーザー情報のみを集約して作られたモデル。
        3.  **ローカルモデル：** 各ユーザー自身のデバイス上のプライベートデータのみでファインチューニングされたモデル。
    *   これらの3つのモデルは、それぞれ重み付け（`αn,1`, `αn,2`, `αn,3`というハイパーパラメータまたは学習可能なパラメータ）をして組み合わされます。
    *   **サンプル計算例（イメージ）：**
        あるユーザーAが映画のレコメンデーションを受けるとします。
        *   ユーザーAのローカルモデルは、Aが過去に見た映画（例えば、SF映画が多い）から学習しています。
        *   ユーザーAが属するクラスタのモデルは、Aと同じような趣味を持つ他のユーザー（例えば、SFとアクション映画が好きな人たち）から学習しています。
        *   グローバルモデルは、システム全体の全てのユーザーの一般的な映画の好み（例えば、大ヒット作）から学習しています。
        最終的なユーザーAのレコメンデーションモデルは、例えば `(0.5 × ローカルモデル) + (0.3 × クラスタモデル) + (0.2 × グローバルモデル)` のように、これら3つのモデルの「意見」を重み付けして組み合わせることで決定されます。これにより、ユーザーAには、彼個人の細かい好み、彼が属するグループの傾向、そして一般的な流行といった様々な側面が考慮された、より精度の高い映画が推薦されることになります。

*   **プライバシー保護（ローカル差分プライバシー, LDP）：**
    *   **なぜ？** 連合学習では、ユーザーのローカルデータ自体は共有されませんが、そのデータから計算された「勾配」（モデルを更新するための情報）がサーバーに送信されます。この勾配から、もし悪意のある第三者が個人の機密情報を推測できてしまうと、プライバシーが侵害される可能性があります。
    *   **どのように機能するのか？** これを防ぐために、**ローカル差分プライバシー (Local Differential Privacy, LDP)**が適用されます。これは、ユーザーデバイスからサーバーに送られる**勾配にランダムなノイズを加える**ことで実現されます。さらに、勾配の大きさが特定の閾値を超えないように「クリッピング」も行われます。
    *   **サンプル計算例（イメージ）：**
        ユーザーのデバイスで計算された「モデル更新のための勾配」が `g_raw = [0.15, -0.08, 0.22]` だったとします。
        1.  **クリッピング（閾値 `δ` = 0.2）:** 各要素の絶対値が閾値を超えた場合、その値を閾値に制限します。
            `g_clipped = [0.15, -0.08, 0.20]` (0.22が0.20にクリップされる)
        2.  **ラプラスノイズの追加（スケール `_` = 0.01）:** 平均ゼロのラプラスノイズを各要素に加えます。例えば、ランダムに `[0.003, -0.001, 0.005]` というノイズが生成されたとします。
            `g_noisy = [0.15+0.003, -0.08-0.001, 0.20+0.005] = [0.153, -0.081, 0.205]`。
        この `g_noisy` がサーバーに送信される勾配となります。このようにノイズを加えることで、送信される情報から元のユーザーデータを正確に特定することが非常に困難になり、プライバシーが保護されます。

#### 3. サーバーサイドのクラスタリングに基づく連合 (Server-Side Clustering-Based Federation)

このモジュールは、中央のサーバー側で行われる処理です。

*   **ユーザーのクラスタリング：**
    *   **なぜ？** 全てのユーザーを同じように扱うのではなく、似たような好みや行動パターンを持つユーザーをグループ化することで、より関連性の高いレコメンデーションを提供できます。また、グループごとに学習することで、ユーザーごとのデータの多様性（ヘテロジニティ）という課題にも対応できます。
    *   **どのように機能するのか？** サーバーは、各ユーザーのデバイスからアップロードされた**学習済み埋め込み（表現）に基づいて**、ユーザーを**類似したK個のグループ（クラスタ）**に分類します。このクラスタリングには、K-meansのような標準的な手法が利用されます。ユーザーの埋め込みは、ユーザーの属性情報と共同作業情報（他のユーザーとのインタラクションパターン）の両方から学習されているため、その表現力は強化されています。
    *   **サンプル計算例（イメージ）：**
        ユーザーの埋め込みが、例えば「映画のジャンル」を表す2次元の座標 `(アクション好き度, コメディ好き度)` のようなものだと想像してみてください。
        *   ユーザーA: (0.8, 0.1) -> アクション好き、コメディはあまり見ない
        *   ユーザーB: (0.9, 0.2) -> アクション好き、コメディはあまり見ない
        *   ユーザーC: (0.2, 0.7) -> アクションはあまり見ない、コメディ好き
        *   ユーザーD: (0.1, 0.6) -> アクションはあまり見ない、コメディ好き
        K-meansクラスタリングを行うと、ユーザーAとBは「アクション映画好き」のクラスタに、ユーザーCとDは「コメディ映画好き」のクラスタに分類されるでしょう。このクラスタリングにより、同じクラスタ内のユーザーは互いに似た傾向を持つと見なされ、そのクラスタに特化したモデルを学習する基盤ができます。

*   **モデルの集約：**
    *   **なぜ？** 各ユーザーはローカルでモデルを訓練しますが、その知識を全体で共有・統合することで、システム全体のモデル性能を向上させることができます。
    *   **どのように機能するのか？** サーバーは、訓練に参加した全ユーザーから受け取った**プライバシー保護された勾配（モデルの更新情報）**を統合し、2種類の集約モデルを生成します。
        1.  **グローバルモデル (Θt_global)：** 全ての参加ユーザーからの勾配を、それぞれのユーザーが持つローカルデータの量（`Dn`）に応じて重み付けして合計することで更新されます。これは、システム全体の「共通の知識」を表すモデルです。
        2.  **クラスタレベルモデル (Θt_C(n))：** 各クラスタ内の参加ユーザーからの勾配のみを、同様に重み付けして合計することで更新されます。これは、特定のユーザーグループの「共通の傾向」を表すモデルです。
    *   これらのグローバルモデルとクラスタレベルモデルは、前述の「ユーザーサイドのローカルレコメンデーションネットワーク」の最後のステップで、各ユーザーにダウンロードされ、そのユーザーのパーソナライズされたレコメンデーションモデルの構築に利用されます。

*   **ユーザー選択：**
    *   **なぜ？** 連合学習では、訓練に参加するユーザーの数を絞ることで、通信コストをさらに削減できます。
    *   **どのように機能するのか？** 従来のランダムなユーザー選択に加え、PerFedRec++では、**「クラスタベースのユーザー選択メカニズム」**を導入しています。これは、各訓練イテレーション（繰り返し）において、**各クラスタ内からクラスタサイズに比例してランダムにユーザーを選択する**方法です。
    *   **サンプル計算例（イメージ）：**
        全体のユーザーが1000人で、2つのクラスタがあったとします。
        *   クラスタ1: ユーザー数 700人
        *   クラスタ2: ユーザー数 300人
        もし、各訓練ラウンドで合計256人のユーザーを選択すると決めた場合、クラスタベースの選択では、クラスタ1からは `256 × (700/1000) = 約179人` を選び、クラスタ2からは `256 × (300/1000) = 約77人` を選びます。
        このように、より多くのユーザーがいるクラスタからより多くの代表的なユーザーを選択することで、訓練効率が向上し、モデルがより多様なユーザーグループの情報を効率的に学習できるようになります。

---

これらのモジュールが連携することで、PerFedRec++はユーザーのプライバシーを尊重しつつ、高速かつ高精度なパーソナライズドレコメンデーションを実現します。